### **Lab: Connecting a Node.js Express Application to MySQL**

Welcome to the next stage of our application development. In the previous lab, you did an excellent job of refactoring the application to separate concerns. However, our data still disappears every time we restart the server because it's stored in an in-memory array.

In this lab, we'll replace our temporary in-memory data store with a persistent MySQL database, a crucial step in building a real-world application.

#### **Prerequisites**

*   Completion of the "Separating Concerns" lab.
*   A running MySQL server on your machine or accessible on your network.
*   A MySQL client to interact with the database (e.g., MySQL Workbench, DBeaver, or the command-line client).

#### **Learning Objectives**

By the end of this lab, you will be able to:

*   Connect a Node.js application to a MySQL database.
*   Understand and implement a connection pool for efficient database communication.
*   Translate data manipulation logic from JavaScript arrays to SQL queries.
*   Refactor a service layer to interact with a database without altering the controller or route layers.

---

### **Part 1: Setting Up the Database and Project**

First, we need to create our database and table. Then we'll install the necessary Node.js package to communicate with MySQL.

**1. Create the Database and Table**

*   Open your MySQL client and connect to your MySQL server.
*   Execute the following SQL commands to create a new database and a `posts` table.

    ```sql
    -- Creates the database.
    CREATE DATABASE blogdatabase;

    -- Selects the new database to use for the next commands.
    USE blogdatabase;

    -- Creates the 'posts' table with an auto-incrementing primary key.
    CREATE TABLE posts (
        id INT AUTO_INCREMENT PRIMARY KEY,
        title VARCHAR(255) NOT NULL,
        content TEXT NOT NULL
    );
    ```

> #### **Code Explanation**
> *   `CREATE DATABASE blogdatabase;`: This command creates a new, empty database named `blogdatabase`. This acts as the container for all our application's tables and data.
> *   `USE blogdatabase;`: This command tells the MySQL client that all subsequent commands should be executed within the `blogdatabase`. It's like changing directories in a terminal.
> *   `CREATE TABLE posts (...)`: This command creates a new table named `posts` where we will store our blog post data.
>     *   `id INT AUTO_INCREMENT PRIMARY KEY`: This defines the `id` column. `INT` is the data type (integer). `AUTO_INCREMENT` tells MySQL to automatically generate a new, unique number for this column every time a row is added. `PRIMARY KEY` designates this column as the unique identifier for each row, which ensures data integrity and makes lookups by ID very fast.
>     *   `title VARCHAR(255) NOT NULL`: This defines the `title` column as a variable-length string with a maximum of 255 characters. `NOT NULL` is a constraint that ensures every post must have a title.
>     *   `content TEXT NOT NULL`: This defines the `content` column using the `TEXT` data type, suitable for long strings like the body of a blog post. The `NOT NULL` constraint ensures every post has content.

**2. Install the MySQL Driver `**

*   In your terminal, at the root of your previous project, install the `mysql2`.

    ```bash
    npm install mysql2
    ```

> #### **Code Explanation**
> *   `npm install`: This is the standard Node.js command to add new packages (libraries) to your project.
> *   `mysql2`: This is the driver that allows your Node.js application to connect to and communicate with a MySQL database. It's a popular and modern replacement for the original `mysql` package, offering better performance and support for modern JavaScript features like Promises.


**3. Configure Environment Variables**

*   Create a new file named `.env` in the root directory of your project. This file will store your database credentials. **Never commit this file to a public repository.**

    ```
    # .env
    DB_HOST=localhost
    DB_USER=your_mysql_username
    DB_PASSWORD=your_mysql_password
    DB_DATABASE=blogdatabase
    ```
    *Replace `your_mysql_username` and `your_mysql_password` with your actual MySQL credentials.*

> #### **Code Explanation**
> The `.env` file stores configuration variables that can differ between environments (development, testing, production). By storing credentials here, you avoid hardcoding them in your application logic. Your `db.js` file will read these values, but the actual password is not visible in the code you might share or commit to version control.

*   To ensure this file isn't tracked by git, create a `.gitignore` file in the root of your project (if you don't have one already) and add `.env` to it.

    ```
    # .gitignore
    node_modules
    .env
    ```

---

### **Part 2: Creating a Database Connection Module**

It's a best practice to manage our database connection in a central place. We will create a configuration file that sets up a **connection pool**, which is a cache of database connections that can be reused. This is much more efficient than opening and closing a connection for every single query.

*   Inside the `src` folder, create a new folder named `config`.
*   Inside `src/config`, create a new file named `db.js`.
*   Add the following code to `src/config/db.js`:

    ```javascript
    // src/config/db.js
    import mysql from 'mysql2/promise';
    import dotenv from 'dotenv';

    dotenv.config(); // Loads environment variables from .env file

    // Create a connection pool
    const pool = mysql.createPool({
        host: process.env.DB_HOST,
        user: process.env.DB_USER,
        password: process.env.DB_PASSWORD,
        database: process.env.DB_DATABASE,
        waitForConnections: true,
        connectionLimit: 10,
        queueLimit: 0
    });

    // A simple function to test the connection
    export const testConnection = async () => {
        try {
            await pool.getConnection();
            console.log('Successfully connected to the MySQL database.');
        } catch (error) {
            console.error('Unable to connect to the database:', error);
        }
    };

    export default pool;
    ```
> #### **Code Explanation**
> *   `import mysql from 'mysql2/promise';`: We specifically import the `promise-wrapper` version of `mysql2`. This allows us to use modern `async/await` syntax, which is much cleaner than traditional callback functions.
> *   `dotenv.config();`: This line executes the `dotenv` library, which reads your `.env` file and makes the variables available under `process.env`. For example, `process.env.DB_HOST` will now hold the value `localhost`.
> *   `mysql.createPool({...});`: This creates the connection pool.
>     *   `host`, `user`, `password`, `database`: These are the connection details, safely loaded from `process.env`.
>     *   `waitForConnections: true`: If all connections in the pool are busy, a new request will wait for a connection to become available instead of immediately failing.
>     *   `connectionLimit: 10`: The pool will maintain a maximum of 10 open connections at any given time.
> *   `export default pool;`: This line makes the configured `pool` object available so it can be imported and used by other files in our application, specifically our service layer.

---

### **Part 3: Refactoring the Service Layer**

This is the core of our migration. We will now modify `src/services/post.service.js` to replace the in-memory array logic with SQL queries. The key benefit of our architecture is that **we only need to change this one file.**

*   Open `src/services/post.service.js` and replace its entire content with the following:

    ```javascript
    // src/services/post.service.js
    import pool from '../config/db.js';

    export const getAllPosts = async () => {
        const [posts] = await pool.query('SELECT * FROM posts');
        return posts;
    };
    ```
> #### **Code Explanation**
> *   `async () => { ... }`: The function is marked as `async` because database operations are asynchronous (they take time to complete), which allows us to use the `await` keyword.
> *   `await pool.query(...)`: This sends the SQL query to the database and pauses execution until the result is returned.
> *   `const [posts] = ...`: The `pool.query` method returns an array `[results, fields]`. The `results` element is the array of data rows we want. Using `[posts]` is JavaScript destructuring to cleanly extract only the data rows into a variable named `posts`.

```javascript
export const getPostById = async (id) => {
    const [rows] = await pool.query('SELECT * FROM posts WHERE id = ?', [id]);
    return rows[0] || null;
};
```
> #### **Code Explanation**
> *   `WHERE id = ?`: This is a **prepared statement**. The `?` is a placeholder for a value.
> *   `[id]`: The second argument to `pool.query` is an array of values that will be safely inserted where the `?` placeholders are. This is a crucial security practice that prevents an attack called **SQL Injection**.
> *   `return rows[0] || null`: The query returns an array of rows. Since `id` is unique, this array will either contain one post object or be empty. We return the first element (`rows[0]`) if it exists, otherwise we return `null`.

```javascript
export const createPost = async (postData) => {
    const { title, content } = postData;
    const [result] = await pool.query(
        'INSERT INTO posts (title, content) VALUES (?, ?)',
            [title, content]
    );
    const newPostId = result.insertId;
    return getPostById(newPostId);
};
```
> #### **Code Explanation**
> *   `INSERT INTO ... VALUES (?, ?)`: The SQL command to create a new row, again using prepared statements for security.
> *   `const [result] = ...`: For an `INSERT` operation, the result object contains metadata about the query, not the data itself.
> *   `result.insertId`: This is a property on the result object that gives us the `id` of the row that was just created by `AUTO_INCREMENT`.
> *   `return getPostById(newPostId)`: To maintain a consistent response, we use the new ID to fetch the complete post object from the database and return it.

```javascript
    export const updatePost = async (id, postData) => {
        const { title, content } = postData;
        const [result] = await pool.query(
            'UPDATE posts SET title = ?, content = ? WHERE id = ?',
            [title, content, id]
        );
        if (result.affectedRows === 0) {
            return null;
        }
        return getPostById(id);
    };

    export const partiallyUpdatePost = async (id, updates) => {
        const fields = Object.keys(updates);
        const values = Object.values(updates);

        if (fields.length === 0) {
            return getPostById(id);
        }
        
        const setClause = fields.map(field => `${field} = ?`).join(', ');
        
        const [result] = await pool.query(
            `UPDATE posts SET ${setClause} WHERE id = ?`,
            [...values, id]
        );

        if (result.affectedRows === 0) {
            return null;
        }
        return getPostById(id);
    };
```
> #### **Code Explanation**
> *   `UPDATE posts SET ...`: The SQL command to modify an existing row.
> *   `result.affectedRows`: The result object for `UPDATE` or `DELETE` queries contains `affectedRows`, which is the number of rows that were changed. If this is `0`, it means no post with the given `id` was found.
> *   The `partiallyUpdatePost` function dynamically builds the `SET` clause, allowing for flexible updates to one or more fields at a time.

```javascript
    export const deletePost = async (id) => {
        const [result] = await pool.query('DELETE FROM posts WHERE id = ?', [id]);
        return result.affectedRows > 0;
    };
```
> #### **Code Explanation**
> *   `DELETE FROM ...`: The SQL command to delete a row.
> *   `return result.affectedRows > 0`: If a post with the given `id` was found and deleted, `affectedRows` will be `1`. If no post with that `id` existed, it will be `0`. This logic allows us to return a simple boolean: `true` for a successful deletion and `false` if the post wasn't found.

---

### **Part 4: Updating the Controller Layer**

Because our service functions are now `async` and return Promises, we need to make small adjustments in our controller functions to `await` their results and handle potential errors.

*   Open `src/controllers/post.controller.js` and update it as follows:

```javascript
    // src/controllers/post.controller.js
    import * as postService from '../services/post.service.js';

    export const getAllPosts = async (req, res) => {
        try {
            const posts = await postService.getAllPosts();
            res.json(posts);
        } catch (error) {
            res.status(500).json({ message: 'Error retrieving posts', error: error.message });
        }
    };

    // (Apply the same async/await and try/catch pattern to all other controller functions:
    // getPostById, createPost, updatePost, partiallyUpdatePost, and deletePost)
```
> #### **Code Explanation**
> The `try...catch` pattern is a standard for modern Express applications.
>
> *   `async (req, res) => { ... }`: The controller function is now `async` because it needs to `await` the result from the service layer.
> *   `try { ... }`: This is the "happy path." We attempt to execute the asynchronous code inside this block.
>     *   `const posts = await postService.getAllPosts();`: We call the service function and `await` its result. The code pauses on this line until the database has responded.
>     *   `res.json(posts);`: If the `await` is successful, we send the retrieved data back to the client.
> *   `catch (error) { ... }`: If anything goes wrong in the `try` block (e.g., the database connection fails), the service function will throw an error. The `catch` block "catches" this error.
>     *   `res.status(500).json(...)`: Instead of crashing the server, we gracefully handle the error by sending a `500 Internal Server Error` status code and a JSON object with an error message. This is crucial for creating a robust API.

---

### **Part 5: Testing the Final Application**

Let's test our work. We'll add our database connection test to the main `index.js` file to ensure the connection is working when the server starts.

*   Open `index.js` and modify it:

    ```javascript
    // index.js
    import express from 'express';
    import postRoutes from './src/routes/post.routes.js';
    import { testConnection } from './src/config/db.js'; // Import the test function

    const app = express();
    const port = 3000;

    app.use(express.json());

    // Mount the post routes
    app.use('/posts', postRoutes);

    app.listen(port, () => {
        console.log(`Server is running on http://localhost:${port}`);
        testConnection(); // Test the database connection on startup
    });
    ```
> #### **Code Explanation**
> *   `import { testConnection } ...`: We import the `testConnection` function we created in our `db.js` file.
> *   `testConnection()`: Calling this function when the server starts provides immediate feedback in the console, confirming whether the application was able to successfully connect to the MySQL database using the credentials from the `.env` file.

*   **Run your server:**
    ```bash
    node index.js
    ```
    You should see two messages in your console:
    `Server is running on http://localhost:3000`
    `Successfully connected to the MySQL database.`

*   **Use Postman or `curl` to test all the endpoints again.** Create, read, update, and delete posts.

*   **Verify in your database:** After testing your API, run `SELECT * FROM posts;` in your MySQL client. You will see the data changes reflected in the table. Now, even if you restart your Node.js server, your data will persist.

Congratulations! You have successfully migrated your application from an in-memory array to a persistent MySQL database, all while maintaining a clean and separated architecture. This is a foundational skill for any backend developer.